{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nLexicon: Collection of words/phrases\\n\\nThree types of Lexicons.\\n\\n1. Mere Wordlist -- Stopwords\\n\\n2. Word list with information --CMU Worldist\\n\\n3. Word list with semantic orientation --- WORDNET\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "'''\nLexicon: Collection of words/phrases\n\nThree types of Lexicons.\n\n1. Mere Wordlist -- Stopwords\n\n2. Word list with information --CMU Worldist\n\n3. Word list with semantic orientation --- WORDNET\n'''"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u\"you're\", u\"you've\", u\"you'll\", u\"you'd\", u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u\"she's\", u'her', u'hers', u'herself', u'it', u\"it's\", u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u\"that'll\", u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u\"don't\", u'should', u\"should've\", u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u\"aren't\", u'couldn', u\"couldn't\", u'didn', u\"didn't\", u'doesn', u\"doesn't\", u'hadn', u\"hadn't\", u'hasn', u\"hasn't\", u'haven', u\"haven't\", u'isn', u\"isn't\", u'ma', u'mightn', u\"mightn't\", u'mustn', u\"mustn't\", u'needn', u\"needn't\", u'shan', u\"shan't\", u'shouldn', u\"shouldn't\", u'wasn', u\"wasn't\", u'weren', u\"weren't\", u'won', u\"won't\", u'wouldn', u\"wouldn't\"]\n"
    }
   ],
   "source": "# Task1: Access a Lexical source\n\n# 1. Stopwords\nimport nltk\nfrom nltk.corpus import stopwords\nprint(stopwords.words('english'))"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[u'au', u'aux', u'avec', u'ce', u'ces', u'dans', u'de', u'des', u'du', u'elle', u'en', u'et', u'eux', u'il', u'ils', u'je', u'la', u'le', u'les', u'leur', u'lui', u'ma', u'mais', u'me', u'm\\xeame', u'mes', u'moi', u'mon', u'ne', u'nos', u'notre', u'nous', u'on', u'ou', u'par', u'pas', u'pour', u'qu', u'que', u'qui', u'sa', u'se', u'ses', u'son', u'sur', u'ta', u'te', u'tes', u'toi', u'ton', u'tu', u'un', u'une', u'vos', u'votre', u'vous', u'c', u'd', u'j', u'l', u'\\xe0', u'm', u'n', u's', u't', u'y', u'\\xe9t\\xe9', u'\\xe9t\\xe9e', u'\\xe9t\\xe9es', u'\\xe9t\\xe9s', u'\\xe9tant', u'\\xe9tante', u'\\xe9tants', u'\\xe9tantes', u'suis', u'es', u'est', u'sommes', u'\\xeates', u'sont', u'serai', u'seras', u'sera', u'serons', u'serez', u'seront', u'serais', u'serait', u'serions', u'seriez', u'seraient', u'\\xe9tais', u'\\xe9tait', u'\\xe9tions', u'\\xe9tiez', u'\\xe9taient', u'fus', u'fut', u'f\\xfbmes', u'f\\xfbtes', u'furent', u'sois', u'soit', u'soyons', u'soyez', u'soient', u'fusse', u'fusses', u'f\\xfbt', u'fussions', u'fussiez', u'fussent', u'ayant', u'ayante', u'ayantes', u'ayants', u'eu', u'eue', u'eues', u'eus', u'ai', u'as', u'avons', u'avez', u'ont', u'aurai', u'auras', u'aura', u'aurons', u'aurez', u'auront', u'aurais', u'aurait', u'aurions', u'auriez', u'auraient', u'avais', u'avait', u'avions', u'aviez', u'avaient', u'eut', u'e\\xfbmes', u'e\\xfbtes', u'eurent', u'aie', u'aies', u'ait', u'ayons', u'ayez', u'aient', u'eusse', u'eusses', u'e\\xfbt', u'eussions', u'eussiez', u'eussent']\n"
    }
   ],
   "source": "print(stopwords.words('french'))"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[u'aber', u'alle', u'allem', u'allen', u'aller', u'alles', u'als', u'also', u'am', u'an', u'ander', u'andere', u'anderem', u'anderen', u'anderer', u'anderes', u'anderm', u'andern', u'anderr', u'anders', u'auch', u'auf', u'aus', u'bei', u'bin', u'bis', u'bist', u'da', u'damit', u'dann', u'der', u'den', u'des', u'dem', u'die', u'das', u'dass', u'da\\xdf', u'derselbe', u'derselben', u'denselben', u'desselben', u'demselben', u'dieselbe', u'dieselben', u'dasselbe', u'dazu', u'dein', u'deine', u'deinem', u'deinen', u'deiner', u'deines', u'denn', u'derer', u'dessen', u'dich', u'dir', u'du', u'dies', u'diese', u'diesem', u'diesen', u'dieser', u'dieses', u'doch', u'dort', u'durch', u'ein', u'eine', u'einem', u'einen', u'einer', u'eines', u'einig', u'einige', u'einigem', u'einigen', u'einiger', u'einiges', u'einmal', u'er', u'ihn', u'ihm', u'es', u'etwas', u'euer', u'eure', u'eurem', u'euren', u'eurer', u'eures', u'f\\xfcr', u'gegen', u'gewesen', u'hab', u'habe', u'haben', u'hat', u'hatte', u'hatten', u'hier', u'hin', u'hinter', u'ich', u'mich', u'mir', u'ihr', u'ihre', u'ihrem', u'ihren', u'ihrer', u'ihres', u'euch', u'im', u'in', u'indem', u'ins', u'ist', u'jede', u'jedem', u'jeden', u'jeder', u'jedes', u'jene', u'jenem', u'jenen', u'jener', u'jenes', u'jetzt', u'kann', u'kein', u'keine', u'keinem', u'keinen', u'keiner', u'keines', u'k\\xf6nnen', u'k\\xf6nnte', u'machen', u'man', u'manche', u'manchem', u'manchen', u'mancher', u'manches', u'mein', u'meine', u'meinem', u'meinen', u'meiner', u'meines', u'mit', u'muss', u'musste', u'nach', u'nicht', u'nichts', u'noch', u'nun', u'nur', u'ob', u'oder', u'ohne', u'sehr', u'sein', u'seine', u'seinem', u'seinen', u'seiner', u'seines', u'selbst', u'sich', u'sie', u'ihnen', u'sind', u'so', u'solche', u'solchem', u'solchen', u'solcher', u'solches', u'soll', u'sollte', u'sondern', u'sonst', u'\\xfcber', u'um', u'und', u'uns', u'unsere', u'unserem', u'unseren', u'unser', u'unseres', u'unter', u'viel', u'vom', u'von', u'vor', u'w\\xe4hrend', u'war', u'waren', u'warst', u'was', u'weg', u'weil', u'weiter', u'welche', u'welchem', u'welchen', u'welcher', u'welches', u'wenn', u'werde', u'werden', u'wie', u'wieder', u'will', u'wir', u'wird', u'wirst', u'wo', u'wollen', u'wollte', u'w\\xfcrde', u'w\\xfcrden', u'zu', u'zum', u'zur', u'zwar', u'zwischen']\n"
    }
   ],
   "source": "print(stopwords.words('german'))"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[(u'belford', [u'B', u'EH1', u'L', u'F', u'ER0', u'D']), (u'belfry', [u'B', u'EH1', u'L', u'F', u'R', u'IY0']), (u'belgacom', [u'B', u'EH1', u'L', u'G', u'AH0', u'K', u'AA0', u'M']), (u'belgacom', [u'B', u'EH1', u'L', u'JH', u'AH0', u'K', u'AA0', u'M']), (u'belgard', [u'B', u'EH0', u'L', u'G', u'AA1', u'R', u'D']), (u'belgarde', [u'B', u'EH0', u'L', u'G', u'AA1', u'R', u'D', u'IY0']), (u'belge', [u'B', u'EH1', u'L', u'JH', u'IY0']), (u'belger', [u'B', u'EH1', u'L', u'G', u'ER0']), (u'belgian', [u'B', u'EH1', u'L', u'JH', u'AH0', u'N']), (u'belgians', [u'B', u'EH1', u'L', u'JH', u'AH0', u'N', u'Z']), (u'belgique', [u'B', u'EH0', u'L', u'ZH', u'IY1', u'K']), (u\"belgique's\", [u'B', u'EH0', u'L', u'JH', u'IY1', u'K', u'S']), (u'belgium', [u'B', u'EH1', u'L', u'JH', u'AH0', u'M']), (u\"belgium's\", [u'B', u'EH1', u'L', u'JH', u'AH0', u'M', u'Z']), (u'belgo', [u'B', u'EH1', u'L', u'G', u'OW2']), (u'belgrade', [u'B', u'EH1', u'L', u'G', u'R', u'EY0', u'D']), (u'belgrade', [u'B', u'EH1', u'L', u'G', u'R', u'AA2', u'D']), (u\"belgrade's\", [u'B', u'EH1', u'L', u'G', u'R', u'EY0', u'D', u'Z']), (u\"belgrade's\", [u'B', u'EH1', u'L', u'G', u'R', u'AA2', u'D', u'Z']), (u'belgrave', [u'B', u'EH1', u'L', u'G', u'R', u'EY2', u'V']), (u'beli', [u'B', u'EH1', u'L', u'IY0']), (u'belich', [u'B', u'EH1', u'L', u'IH0', u'K']), (u'belie', [u'B', u'IH0', u'L', u'AY1']), (u'belied', [u'B', u'IH0', u'L', u'AY1', u'D']), (u'belief', [u'B', u'IH0', u'L', u'IY1', u'F'])]\n"
    }
   ],
   "source": "#2. CMP Wordlist\nimport nltk\nentries = nltk.corpus.cmudict.entries()\nlen(entries)\nprint(entries[10000:10025])"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[Synset('car.n.01')]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "#3. Wordnet\nfrom nltk.corpus import wordnet as wn\nwn.synsets('motorcar') # You get an id for subsets\n"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[u'car', u'auto', u'automobile', u'machine', u'motorcar']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "wn.synset('car.n.01').lemma_names() # head words/lemmas in the subset"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[Synset('natural_language_processing.n.01')]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "wn.synsets('nlp')"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[u'natural_language_processing', u'NLP', u'human_language_technology']"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "wn.synset('natural_language_processing.n.01').lemma_names()"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[('Diesel', 'NNP'), ('began', 'VBD'), ('his', 'PRP$'), ('career', 'NN'), ('in', 'IN'), ('1990', 'CD'), (',', ','), ('but', 'CC'), ('initially', 'RB'), ('struggled', 'VBD'), ('to', 'TO'), ('gain', 'VB'), ('roles', 'NNS'), ('until', 'IN'), ('he', 'PRP'), ('created', 'VBD'), ('and', 'CC'), ('starred', 'VBD'), ('in', 'IN'), ('the', 'DT'), ('short', 'JJ'), ('film', 'NN'), ('Multi-Facial', 'NNP'), ('(', '('), ('1995', 'CD'), (')', ')'), (',', ','), ('which', 'WDT'), ('attracted', 'VBD'), ('the', 'DT'), ('attention', 'NN'), ('of', 'IN'), ('director', 'NN'), ('Steven', 'NNP'), ('Spielberg', 'NNP'), (',', ','), ('who', 'WP'), ('was', 'VBD'), ('developing', 'VBG'), ('his', 'PRP$'), ('film', 'NN'), ('Saving', 'VBG'), ('Private', 'NNP'), ('Ryan', 'NNP'), ('.', '.')]\n[('Spielberg', 'NNP'), ('re-wrote', 'JJ'), ('elements', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('film', 'NN'), ('to', 'TO'), ('allow', 'VB'), ('Diesel', 'NNP'), ('to', 'TO'), ('appear', 'VB'), ('in', 'IN'), ('it', 'PRP'), (',', ','), ('which', 'WDT'), ('helped', 'VBD'), ('kickstart', 'VB'), ('his', 'PRP$'), ('career', 'NN'), ('.', '.')]\n[('He', 'PRP'), ('subsequently', 'RB'), ('voiced', 'VBD'), ('the', 'DT'), ('title', 'NN'), ('character', 'NN'), ('in', 'IN'), ('The', 'DT'), ('Iron', 'NNP'), ('Giant', 'NNP'), ('(', '('), ('1999', 'CD'), (')', ')'), ('and', 'CC'), ('gained', 'VBD'), ('a', 'DT'), ('reputation', 'NN'), ('as', 'IN'), ('an', 'DT'), ('action', 'NN'), ('star', 'NN'), ('after', 'IN'), ('starring', 'VBG'), ('in', 'IN'), ('The', 'DT'), ('Fast', 'NNP'), ('and', 'CC'), ('the', 'DT'), ('Furious', 'NNP'), (',', ','), ('The', 'DT'), ('Chronicles', 'NNP'), ('of', 'IN'), ('Riddick', 'NNP'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('XXX', 'NNP'), ('series', 'NN'), ('.', '.')]\n[('Later', 'RB'), ('in', 'IN'), ('his', 'PRP$'), ('career', 'NN'), (',', ','), ('Diesel', 'NNP'), ('also', 'RB'), ('became', 'VBD'), ('known', 'VBN'), ('for', 'IN'), ('lending', 'VBG'), ('his', 'PRP$'), ('voice', 'NN'), ('to', 'TO'), ('the', 'DT'), ('character', 'NN'), ('of', 'IN'), ('Groot', 'NNP'), ('in', 'IN'), ('Guardians', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Galaxy', 'NNP'), ('(', '('), ('2014', 'CD'), (')', ')'), ('and', 'CC'), ('voicing', 'VBG'), ('a', 'DT'), ('character', 'NN'), ('in', 'IN'), ('Ralph', 'NNP'), ('Breaks', 'NNP'), ('the', 'DT'), ('Internet', 'NNP'), ('(', '('), ('2018', 'CD'), (')', ')'), ('.', '.')]\n[('He', 'PRP'), ('also', 'RB'), ('founded', 'VBD'), ('the', 'DT'), ('production', 'NN'), ('company', 'NN'), ('One', 'CD'), ('Race', 'NNP'), ('Films', 'NNP'), ('.', '.')]\n[('Away', 'RB'), ('from', 'IN'), ('action', 'NN'), ('films', 'NNS'), (',', ','), ('Diesel', 'NNP'), ('has', 'VBZ'), ('enjoyed', 'VBN'), ('commercial', 'JJ'), ('success', 'NN'), ('in', 'IN'), ('a', 'DT'), ('number', 'NN'), ('of', 'IN'), ('other', 'JJ'), ('genres', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('in', 'IN'), ('the', 'DT'), ('comedy', 'NN'), ('film', 'NN'), ('The', 'DT'), ('Pacifier', 'NNP'), ('(', '('), ('2005', 'CD'), (')', ')'), (',', ','), ('while', 'IN'), ('his', 'PRP$'), ('performance', 'NN'), ('in', 'IN'), ('Find', 'NNP'), ('Me', 'NNP'), ('Guilty', 'NNP'), ('(', '('), ('2006', 'CD'), (')', ')'), ('was', 'VBD'), ('praised', 'VBN'), ('.', '.')]\n"
    }
   ],
   "source": "# NLP Pipeline\n\n# \"\"\"\"\"\" Used to enter multiline string\ntexts = [\"\"\"\nDiesel began his career in 1990, but initially struggled to gain roles until he created and starred in the short film Multi-Facial (1995), which attracted the attention of director Steven Spielberg, who was developing his film Saving Private Ryan. Spielberg re-wrote elements of the film to allow Diesel to appear in it, which helped kickstart his career. He subsequently voiced the title character in The Iron Giant (1999) and gained a reputation as an action star after starring in The Fast and the Furious, The Chronicles of Riddick, and the XXX series.\nLater in his career, Diesel also became known for lending his voice to the character of Groot in Guardians of the Galaxy (2014) and voicing a character in Ralph Breaks the Internet (2018). He also founded the production company One Race Films. Away from action films, Diesel has enjoyed commercial success in a number of other genres, such as in the comedy film The Pacifier (2005), while his performance in Find Me Guilty (2006) was praised.\n\"\"\"]\n\nfor text in texts:\n         # Sentence tokenization\n         sentences = nltk.sent_tokenize(text)\n         for sentence in sentences:\n             # Word tokenization\n             words = nltk.word_tokenize(sentence)\n             tagged_words = nltk.pos_tag(words)\n             print(tagged_words)"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[u'The', u'party', u':)', u'was', u'sooo', u'fun', u':D', u'#superfun', u':/']"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "# Task 3\n# Tweet tokenizer\n\nfrom nltk.tokenize import TweetTokenizer\ntext = 'The party :) was sooo fun :D #superfun :/' # Take any real time tweet with hashtags and emojis\ntwtkn = TweetTokenizer()\ntwtkn.tokenize(text)"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[u'If',\n u'you',\n u'say',\n u'this',\n u'youre',\n u'probably',\n u'immature',\n u'XD',\n u':P',\n u'Lets',\n u'be',\n u'honest',\n u'lets',\n u'just',\n u'be',\n u'real',\n u':D']"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "text2 = 'If you say this youre probably immature XD :P Lets be honest lets just be real :D'\ntwtkn.tokenize(text2)"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "('can', ':', 94)\n('could', ':', 87)\n('may', ':', 93)\n('might', ':', 38)\n('must', ':', 53)\n('will', ':', 389)\n"
    }
   ],
   "source": "# Task 4\n# Frequency Distriution\n\n# Brown Corpus\nfrom nltk.corpus import brown\nnews_text = brown.words(categories='news')\nfdist = nltk.FreqDist(w.lower() for w in news_text)\n\nmodals = ['can', 'could', 'may', 'might', 'must', 'will']\nfor m in modals:\n    print(m, ':', fdist[m])"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "('fellow', ':', 8)\n('citizens', ':', 11)\n('india', ':', 5)\n('nation', ':', 13)\n"
    }
   ],
   "source": "speech = ['fellow', 'citizens', 'india', 'nation']\nfor s in speech:\n    print(s, ':', fdist[s])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
